// SPDX-License-Identifier: MIT
// Copyright 2025 - Present, Light Transport Entertainment Inc.
//
// Highly Optimized Cooperative Matrix Matmul for AMD RDNA4
// Uses VK_KHR_cooperative_matrix for hardware-accelerated matrix multiply
//
// Optimizations:
// - 128x64 output tiles for high arithmetic intensity
// - Deep K-tiling (64 elements) for better L2 cache utilization
// - Fully unrolled inner loops
// - Vectorized 128-bit loads (f16vec4)
// - Optimized shared memory layout with XOR swizzle pattern
// - 256 threads / 4 subgroups per workgroup
//
// C = A * B where A[M,K], B[K,N], C[M,N] (row-major)
//
#version 450

#extension GL_KHR_cooperative_matrix : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_EXT_shader_16bit_storage : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_control_flow_attributes : enable

// Cooperative matrix tile dimensions (16x16x16 for RDNA4)
#define CM_M 16
#define CM_N 16
#define CM_K 16

// Workgroup output tile dimensions
#define BM 128      // Output rows per workgroup
#define BN 64       // Output cols per workgroup
#define BK 64       // K-depth per iteration (4x CM_K)

// Thread configuration
#define BLOCK_SIZE 256

layout(local_size_x = BLOCK_SIZE, local_size_y = 1, local_size_z = 1) in;

// Storage buffers
layout(std430, binding = 0) readonly buffer MatrixA { float16_t A[]; };
layout(std430, binding = 1) readonly buffer MatrixB { float16_t B[]; };
layout(std430, binding = 2) writeonly buffer MatrixC { float16_t C[]; };

// Vectorized views
layout(std430, binding = 0) readonly buffer MatrixAVec4 { f16vec4 A_vec4[]; };
layout(std430, binding = 1) readonly buffer MatrixBVec4 { f16vec4 B_vec4[]; };

layout(push_constant) uniform PushConstants {
    uint M;
    uint N;
    uint K;
} pc;

// Shared memory with bank-conflict-avoiding stride
// XOR-based swizzle: stride = BK + (BK/4) to spread accesses across banks
#define A_STRIDE (BK + 16)
#define B_STRIDE (BN + 16)

shared float16_t tileA[BM][A_STRIDE];
shared float16_t tileB[BK][B_STRIDE];

void main() {
    const uint tid = gl_LocalInvocationIndex;
    const uint subgroup_id = gl_SubgroupID;
    const uint num_subgroups = gl_NumSubgroups;

    // Block position
    const uint block_row = gl_WorkGroupID.y * BM;
    const uint block_col = gl_WorkGroupID.x * BN;

    // Subgroup tile assignment
    // 4 subgroups: each handles 32x32 in a 4x2 grid (128x64 total)
    // 8 subgroups: each handles 16x32 in a 8x2 grid
    uint warp_row, warp_col;
    uint tiles_m, tiles_n;

    if (num_subgroups <= 4) {
        // 64-lane waves: 4 subgroups in 4x1 column arrangement for BM=128
        warp_row = subgroup_id * 32;
        warp_col = 0;
        tiles_m = 2;
        tiles_n = 4;  // Each subgroup does 32x64 = 2x4 tiles
    } else {
        // 32-lane waves: 8 subgroups in 8x1 arrangement
        warp_row = subgroup_id * 16;
        warp_col = 0;
        tiles_m = 1;
        tiles_n = 4;
    }

    // Accumulators - 2x4 tiles per subgroup for max register usage
    coopmat<float, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator> acc[2][4];
    [[unroll]] for (uint i = 0; i < 2; i++) {
        [[unroll]] for (uint j = 0; j < 4; j++) {
            acc[i][j] = coopmat<float, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator>(0.0);
        }
    }

    // Precompute loading indices for A
    // 256 threads, BM*BK = 128*64 = 8192 elements, 32 per thread
    const uint a_elem_per_thread = (BM * BK) / BLOCK_SIZE;  // 32
    const uint a_vec4_per_thread = a_elem_per_thread / 4;   // 8

    // Precompute loading indices for B
    // 256 threads, BK*BN = 64*64 = 4096 elements, 16 per thread
    const uint b_elem_per_thread = (BK * BN) / BLOCK_SIZE;  // 16
    const uint b_vec4_per_thread = b_elem_per_thread / 4;   // 4

    // Main K-loop
    for (uint k_base = 0; k_base < pc.K; k_base += BK) {

        // === Load A tile [BM x BK] = [128 x 64] ===
        [[unroll]] for (uint v = 0; v < a_vec4_per_thread; v++) {
            uint vec_idx = tid * a_vec4_per_thread + v;
            uint elem_idx = vec_idx * 4;
            uint row = elem_idx / BK;
            uint col = elem_idx % BK;

            uint gr = block_row + row;
            uint gc = k_base + col;

            f16vec4 data;
            if (gr < pc.M && gc + 3 < pc.K && ((gc & 3) == 0)) {
                data = A_vec4[(gr * pc.K + gc) >> 2];
            } else {
                data.x = (gr < pc.M && gc + 0 < pc.K) ? A[gr * pc.K + gc + 0] : float16_t(0);
                data.y = (gr < pc.M && gc + 1 < pc.K) ? A[gr * pc.K + gc + 1] : float16_t(0);
                data.z = (gr < pc.M && gc + 2 < pc.K) ? A[gr * pc.K + gc + 2] : float16_t(0);
                data.w = (gr < pc.M && gc + 3 < pc.K) ? A[gr * pc.K + gc + 3] : float16_t(0);
            }
            tileA[row][col + 0] = data.x;
            tileA[row][col + 1] = data.y;
            tileA[row][col + 2] = data.z;
            tileA[row][col + 3] = data.w;
        }

        // === Load B tile [BK x BN] = [64 x 64] ===
        [[unroll]] for (uint v = 0; v < b_vec4_per_thread; v++) {
            uint vec_idx = tid * b_vec4_per_thread + v;
            uint elem_idx = vec_idx * 4;
            uint row = elem_idx / BN;
            uint col = elem_idx % BN;

            uint gr = k_base + row;
            uint gc = block_col + col;

            f16vec4 data;
            if (gr < pc.K && gc + 3 < pc.N && ((gc & 3) == 0)) {
                data = B_vec4[(gr * pc.N + gc) >> 2];
            } else {
                data.x = (gr < pc.K && gc + 0 < pc.N) ? B[gr * pc.N + gc + 0] : float16_t(0);
                data.y = (gr < pc.K && gc + 1 < pc.N) ? B[gr * pc.N + gc + 1] : float16_t(0);
                data.z = (gr < pc.K && gc + 2 < pc.N) ? B[gr * pc.N + gc + 2] : float16_t(0);
                data.w = (gr < pc.K && gc + 3 < pc.N) ? B[gr * pc.N + gc + 3] : float16_t(0);
            }
            tileB[row][col + 0] = data.x;
            tileB[row][col + 1] = data.y;
            tileB[row][col + 2] = data.z;
            tileB[row][col + 3] = data.w;
        }

        barrier();

        // === Multiply-Accumulate: 4 iterations of K=16 ===
        [[unroll]] for (uint k = 0; k < BK; k += CM_K) {
            // Load A fragments
            coopmat<float16_t, gl_ScopeSubgroup, CM_M, CM_K, gl_MatrixUseA> fragA[2];
            [[unroll]] for (uint mi = 0; mi < tiles_m; mi++) {
                coopMatLoad(fragA[mi], tileA[warp_row + mi * CM_M], k, A_STRIDE, gl_CooperativeMatrixLayoutRowMajor);
            }

            // Load B fragments and accumulate
            [[unroll]] for (uint ni = 0; ni < tiles_n; ni++) {
                coopmat<float16_t, gl_ScopeSubgroup, CM_K, CM_N, gl_MatrixUseB> fragB;
                coopMatLoad(fragB, tileB[k], warp_col + ni * CM_N, B_STRIDE, gl_CooperativeMatrixLayoutRowMajor);

                [[unroll]] for (uint mi = 0; mi < tiles_m; mi++) {
                    acc[mi][ni] = coopMatMulAdd(fragA[mi], fragB, acc[mi][ni]);
                }
            }
        }

        barrier();
    }

    // === Store results ===
    [[unroll]] for (uint mi = 0; mi < tiles_m; mi++) {
        [[unroll]] for (uint ni = 0; ni < tiles_n; ni++) {
            uint c_row = block_row + warp_row + mi * CM_M;
            uint c_col = block_col + warp_col + ni * CM_N;

            if (c_row + CM_M <= pc.M && c_col + CM_N <= pc.N) {
                coopmat<float16_t, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator> result;
                result = coopmat<float16_t, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator>(acc[mi][ni]);
                coopMatStore(result, C, c_row * pc.N + c_col, pc.N, gl_CooperativeMatrixLayoutRowMajor);
            } else if (c_row < pc.M && c_col < pc.N) {
                coopmat<float16_t, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator> result;
                result = coopmat<float16_t, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator>(acc[mi][ni]);
                coopMatStore(result, C, c_row * pc.N + c_col, pc.N, gl_CooperativeMatrixLayoutRowMajor);
            }
        }
    }
}
