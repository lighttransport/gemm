/*
 * cuda_kernels_common.h - Shared CUDA kernel source string for DA3/PPD runners
 *
 * Contains 14 shared kernels as a C string literal for NVRTC compilation.
 * Each runner concatenates this with its model-specific kernels before NVRTC.
 *
 * Includes extern "C" { at start. The runner's specific kernel string
 * must include the closing } at its end.
 *
 * IMPORTANT: layernorm_f32 includes the __syncthreads() fix after reading
 * sdata[0] to prevent race conditions on sm_120 (Blackwell).
 */
#ifndef CUDA_KERNELS_COMMON_H
#define CUDA_KERNELS_COMMON_H

static const char cuda_kernels_common_src[] =
"typedef unsigned short half_raw;\n"
"__device__ __forceinline__ float half_to_float(half_raw h) {\n"
"    float f; asm(\"cvt.f32.f16 %0, %1;\" : \"=f\"(f) : \"h\"(h)); return f;\n"
"}\n"
"\n"
"extern \"C\" {\n"
"\n"
"/* ---- layernorm_f32 (with __syncthreads fix for sm_120) ---- */\n"
"__global__ void layernorm_f32(float *dst, const float *src, const float *w,\n"
"                               const float *b, int dim, float eps) {\n"
"    extern __shared__ float sdata[];\n"
"    int tok = blockIdx.x;\n"
"    int tid = threadIdx.x;\n"
"    int nt = blockDim.x;\n"
"    const float *x = src + tok * dim;\n"
"    float *y = dst + tok * dim;\n"
"    float s = 0.0f;\n"
"    for (int i = tid; i < dim; i += nt) s += x[i];\n"
"    sdata[tid] = s;\n"
"    __syncthreads();\n"
"    for (int r = nt/2; r > 0; r >>= 1) { if (tid < r) sdata[tid] += sdata[tid+r]; __syncthreads(); }\n"
"    float mean = sdata[0] / (float)dim;\n"
"    __syncthreads();\n"
"    s = 0.0f;\n"
"    for (int i = tid; i < dim; i += nt) { float d = x[i] - mean; s += d*d; }\n"
"    sdata[tid] = s;\n"
"    __syncthreads();\n"
"    for (int r = nt/2; r > 0; r >>= 1) { if (tid < r) sdata[tid] += sdata[tid+r]; __syncthreads(); }\n"
"    float inv = rsqrtf(sdata[0] / (float)dim + eps);\n"
"    for (int i = tid; i < dim; i += nt)\n"
"        y[i] = (x[i] - mean) * inv * w[i] + b[i];\n"
"}\n"
"\n"
"/* ---- gemm_f16_f32: MMA m16n8k16 ---- */\n"
"#define GEMM_N_TILE 8\n"
"__global__ void gemm_f16_f32(float *Y, const half_raw *W, const float *X,\n"
"                              const float *bias,\n"
"                              int n_out, int n_in, int n_tok) {\n"
"    extern __shared__ float smem_x[];\n"
"    int tok_base = blockIdx.y * 16;\n"
"    int warp_id = threadIdx.x / 32;\n"
"    int out_base = blockIdx.x * 256 + warp_id * 64;\n"
"    int lane = threadIdx.x % 32;\n"
"    int gid = lane / 4;\n"
"    int tid4 = lane % 4;\n"
"    int tid = threadIdx.x;\n"
"\n"
"    if (tok_base >= n_tok) return;\n"
"\n"
"    float d0[GEMM_N_TILE], d1[GEMM_N_TILE], d2[GEMM_N_TILE], d3[GEMM_N_TILE];\n"
"#pragma unroll\n"
"    for (int i = 0; i < GEMM_N_TILE; i++) { d0[i]=0; d1[i]=0; d2[i]=0; d3[i]=0; }\n"
"\n"
"    for (int k = 0; k < n_in; k += 16) {\n"
"        int srow = tid / 8, scol = (tid % 8) * 2;\n"
"        int grow = tok_base + srow;\n"
"        if (grow < n_tok) {\n"
"            smem_x[srow * 16 + scol] = X[grow * n_in + k + scol];\n"
"            smem_x[srow * 16 + scol + 1] = X[grow * n_in + k + scol + 1];\n"
"        } else {\n"
"            smem_x[srow * 16 + scol] = 0.0f;\n"
"            smem_x[srow * 16 + scol + 1] = 0.0f;\n"
"        }\n"
"        __syncthreads();\n"
"\n"
"        unsigned int a0, a1, a2, a3;\n"
"        { float f0 = smem_x[gid * 16 + tid4 * 2];\n"
"          float f1 = smem_x[gid * 16 + tid4 * 2 + 1];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a0) : \"f\"(f0), \"f\"(f1)); }\n"
"#if __CUDA_ARCH__ >= 1200\n"
"        /* Blackwell sm_120+: a1/a2 fragment mapping is swapped vs sm_70-90 */\n"
"        { float f0 = smem_x[(gid + 8) * 16 + tid4 * 2];\n"
"          float f1 = smem_x[(gid + 8) * 16 + tid4 * 2 + 1];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a1) : \"f\"(f0), \"f\"(f1)); }\n"
"        { float f0 = smem_x[gid * 16 + tid4 * 2 + 8];\n"
"          float f1 = smem_x[gid * 16 + tid4 * 2 + 9];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a2) : \"f\"(f0), \"f\"(f1)); }\n"
"#else\n"
"        { float f0 = smem_x[gid * 16 + tid4 * 2 + 8];\n"
"          float f1 = smem_x[gid * 16 + tid4 * 2 + 9];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a1) : \"f\"(f0), \"f\"(f1)); }\n"
"        { float f0 = smem_x[(gid + 8) * 16 + tid4 * 2];\n"
"          float f1 = smem_x[(gid + 8) * 16 + tid4 * 2 + 1];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a2) : \"f\"(f0), \"f\"(f1)); }\n"
"#endif\n"
"        { float f0 = smem_x[(gid + 8) * 16 + tid4 * 2 + 8];\n"
"          float f1 = smem_x[(gid + 8) * 16 + tid4 * 2 + 9];\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\" : \"=r\"(a3) : \"f\"(f0), \"f\"(f1)); }\n"
"\n"
"#pragma unroll\n"
"        for (int nt = 0; nt < GEMM_N_TILE; nt++) {\n"
"            int bc = out_base + nt * 8 + gid;\n"
"            unsigned int b0 = 0, b1 = 0;\n"
"            if (bc < n_out) {\n"
"                const half_raw *wp = W + (size_t)bc * n_in + k;\n"
"                b0 = *(const unsigned int *)(wp + tid4 * 2);\n"
"                b1 = *(const unsigned int *)(wp + tid4 * 2 + 8);\n"
"            }\n"
"            asm volatile(\n"
"                \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\\n\\t\"\n"
"                \"    {%0,%1,%2,%3}, {%4,%5,%6,%7}, {%8,%9}, {%10,%11,%12,%13};\"\n"
"                : \"=f\"(d0[nt]), \"=f\"(d1[nt]), \"=f\"(d2[nt]), \"=f\"(d3[nt])\n"
"                : \"r\"(a0), \"r\"(a1), \"r\"(a2), \"r\"(a3),\n"
"                  \"r\"(b0), \"r\"(b1),\n"
"                  \"f\"(d0[nt]), \"f\"(d1[nt]), \"f\"(d2[nt]), \"f\"(d3[nt])\n"
"            );\n"
"        }\n"
"        __syncthreads();\n"
"    }\n"
"\n"
"    int yr0 = tok_base + gid;\n"
"    int yr1 = tok_base + gid + 8;\n"
"#pragma unroll\n"
"    for (int nt = 0; nt < GEMM_N_TILE; nt++) {\n"
"        int yc0 = out_base + nt * 8 + tid4 * 2;\n"
"        int yc1 = yc0 + 1;\n"
"        float bv0 = (bias && yc0 < n_out) ? bias[yc0] : 0.0f;\n"
"        float bv1 = (bias && yc1 < n_out) ? bias[yc1] : 0.0f;\n"
"        if (yr0 < n_tok && yc0 < n_out) Y[yr0 * n_out + yc0] = d0[nt] + bv0;\n"
"        if (yr0 < n_tok && yc1 < n_out) Y[yr0 * n_out + yc1] = d1[nt] + bv1;\n"
"        if (yr1 < n_tok && yc0 < n_out) Y[yr1 * n_out + yc0] = d2[nt] + bv0;\n"
"        if (yr1 < n_tok && yc1 < n_out) Y[yr1 * n_out + yc1] = d3[nt] + bv1;\n"
"    }\n"
"}\n"
"\n"
"/* ---- gemm_fp8_f32: FP8 E4M3 MMA m16n8k32 (sm_89+) ---- */\n"
"#if __CUDA_ARCH__ >= 890\n"
"__global__ void gemm_fp8_f32(float *Y, const unsigned char *W, const float *X,\n"
"                              const float *bias, int n_out, int n_in, int n_tok) {\n"
"    extern __shared__ float smem_x[];\n"
"    int tok_base = blockIdx.y * 16;\n"
"    int warp_id = threadIdx.x / 32;\n"
"    int out_base = blockIdx.x * 256 + warp_id * 64;\n"
"    int lane = threadIdx.x % 32;\n"
"    int gid = lane / 4;\n"
"    int tid4 = lane % 4;\n"
"    int tid = threadIdx.x;\n"
"\n"
"    if (tok_base >= n_tok) return;\n"
"\n"
"    float d0[GEMM_N_TILE], d1[GEMM_N_TILE], d2[GEMM_N_TILE], d3[GEMM_N_TILE];\n"
"#pragma unroll\n"
"    for (int i = 0; i < GEMM_N_TILE; i++) { d0[i]=0; d1[i]=0; d2[i]=0; d3[i]=0; }\n"
"\n"
"    for (int k = 0; k < n_in; k += 32) {\n"
"        int srow = tid / 8, scol = (tid % 8) * 4;\n"
"        int grow = tok_base + srow;\n"
"        if (grow < n_tok) {\n"
"            smem_x[srow * 32 + scol]     = X[grow * n_in + k + scol];\n"
"            smem_x[srow * 32 + scol + 1] = X[grow * n_in + k + scol + 1];\n"
"            smem_x[srow * 32 + scol + 2] = X[grow * n_in + k + scol + 2];\n"
"            smem_x[srow * 32 + scol + 3] = X[grow * n_in + k + scol + 3];\n"
"        } else {\n"
"            smem_x[srow * 32 + scol]     = 0.0f;\n"
"            smem_x[srow * 32 + scol + 1] = 0.0f;\n"
"            smem_x[srow * 32 + scol + 2] = 0.0f;\n"
"            smem_x[srow * 32 + scol + 3] = 0.0f;\n"
"        }\n"
"        __syncthreads();\n"
"\n"
"        unsigned int a0, a1, a2, a3;\n"
"        { unsigned short lo, hi;\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(lo) : \"f\"(smem_x[gid * 32 + tid4 * 4]), \"f\"(smem_x[gid * 32 + tid4 * 4 + 1]));\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(hi) : \"f\"(smem_x[gid * 32 + tid4 * 4 + 2]), \"f\"(smem_x[gid * 32 + tid4 * 4 + 3]));\n"
"          a0 = (unsigned int)lo | ((unsigned int)hi << 16); }\n"
"        { unsigned short lo, hi;\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(lo) : \"f\"(smem_x[gid * 32 + tid4 * 4 + 16]), \"f\"(smem_x[gid * 32 + tid4 * 4 + 17]));\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(hi) : \"f\"(smem_x[gid * 32 + tid4 * 4 + 18]), \"f\"(smem_x[gid * 32 + tid4 * 4 + 19]));\n"
"          a1 = (unsigned int)lo | ((unsigned int)hi << 16); }\n"
"        { unsigned short lo, hi;\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(lo) : \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4]), \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 1]));\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(hi) : \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 2]), \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 3]));\n"
"          a2 = (unsigned int)lo | ((unsigned int)hi << 16); }\n"
"        { unsigned short lo, hi;\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(lo) : \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 16]), \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 17]));\n"
"          asm(\"cvt.rn.satfinite.e4m3x2.f32 %0, %2, %1;\" : \"=h\"(hi) : \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 18]), \"f\"(smem_x[(gid + 8) * 32 + tid4 * 4 + 19]));\n"
"          a3 = (unsigned int)lo | ((unsigned int)hi << 16); }\n"
"\n"
"#pragma unroll\n"
"        for (int nt = 0; nt < GEMM_N_TILE; nt++) {\n"
"            int bc = out_base + nt * 8 + gid;\n"
"            unsigned int b0 = 0, b1 = 0;\n"
"            if (bc < n_out) {\n"
"                const unsigned char *wp = W + (size_t)bc * n_in + k;\n"
"                b0 = *(const unsigned int *)(wp + tid4 * 4);\n"
"                b1 = *(const unsigned int *)(wp + tid4 * 4 + 16);\n"
"            }\n"
"            asm volatile(\n"
"                \"mma.sync.aligned.m16n8k32.row.col.f32.e4m3.e4m3.f32\\n\\t\"\n"
"                \"    {%0,%1,%2,%3}, {%4,%5,%6,%7}, {%8,%9}, {%10,%11,%12,%13};\"\n"
"                : \"=f\"(d0[nt]), \"=f\"(d1[nt]), \"=f\"(d2[nt]), \"=f\"(d3[nt])\n"
"                : \"r\"(a0), \"r\"(a1), \"r\"(a2), \"r\"(a3),\n"
"                  \"r\"(b0), \"r\"(b1),\n"
"                  \"f\"(d0[nt]), \"f\"(d1[nt]), \"f\"(d2[nt]), \"f\"(d3[nt])\n"
"            );\n"
"        }\n"
"        __syncthreads();\n"
"    }\n"
"\n"
"    int yr0 = tok_base + gid;\n"
"    int yr1 = tok_base + gid + 8;\n"
"#pragma unroll\n"
"    for (int nt = 0; nt < GEMM_N_TILE; nt++) {\n"
"        int yc0 = out_base + nt * 8 + tid4 * 2;\n"
"        int yc1 = yc0 + 1;\n"
"        float bv0 = (bias && yc0 < n_out) ? bias[yc0] : 0.0f;\n"
"        float bv1 = (bias && yc1 < n_out) ? bias[yc1] : 0.0f;\n"
"        if (yr0 < n_tok && yc0 < n_out) Y[yr0 * n_out + yc0] = d0[nt] + bv0;\n"
"        if (yr0 < n_tok && yc1 < n_out) Y[yr0 * n_out + yc1] = d1[nt] + bv1;\n"
"        if (yr1 < n_tok && yc0 < n_out) Y[yr1 * n_out + yc0] = d2[nt] + bv0;\n"
"        if (yr1 < n_tok && yc1 < n_out) Y[yr1 * n_out + yc1] = d3[nt] + bv1;\n"
"    }\n"
"}\n"
"#endif\n"
"\n"
"/* ---- add_bias_f32 ---- */\n"
"__global__ void add_bias_f32(float *Y, const float *bias, int n_out, int n_tok) {\n"
"    int i = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (i < n_out * n_tok) Y[i] += bias[i % n_out];\n"
"}\n"
"\n"
"/* ---- attn_prefill_f32: Tensor Core FlashAttention, online softmax ---- */\n"
"__global__ void attn_prefill_f32(float *out, const float *qkv,\n"
"                                  const float *K_t, const float *V_t,\n"
"                                  int n_tok, int dim, int n_heads, int head_dim,\n"
"                                  float scale) {\n"
"    int h = blockIdx.x;\n"
"    if (h >= n_heads) return;\n"
"    int warp_id = threadIdx.x / 32;\n"
"    int qb = blockIdx.y * 64 + warp_id * 16;\n"
"    if (qb >= n_tok) return;\n"
"    int lane = threadIdx.x % 32;\n"
"    int gid = lane / 4, tid4 = lane % 4;\n"
"    int dim3 = 3 * dim;\n"
"    const float *kt_h = K_t + h * n_tok * head_dim;\n"
"    const float *vt_h = V_t + h * n_tok * head_dim;\n"
"    int qi0 = qb + gid, qi1 = qb + gid + 8;\n"
"\n"
"    unsigned int qa0[4], qa1[4], qa2[4], qa3[4];\n"
"#pragma unroll\n"
"    for (int kk = 0; kk < 64; kk += 16) {\n"
"        int ks = kk / 16;\n"
"        int dc = kk + tid4*2;\n"
"        { float f0=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc]:0, f1=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc+1]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa0[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"#if __CUDA_ARCH__ >= 1200\n"
"        /* Blackwell sm_120+: a1/a2 fragment mapping is swapped vs sm_70-90 */\n"
"        { float f0=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc]:0, f1=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc+1]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa1[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"        { float f0=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc+8]:0, f1=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc+9]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa2[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"#else\n"
"        { float f0=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc+8]:0, f1=(qi0<n_tok)?qkv[qi0*dim3+h*head_dim+dc+9]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa1[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"        { float f0=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc]:0, f1=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc+1]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa2[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"#endif\n"
"        { float f0=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc+8]:0, f1=(qi1<n_tok)?qkv[qi1*dim3+h*head_dim+dc+9]:0;\n"
"          asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(qa3[ks]):\"f\"(f0),\"f\"(f1)); }\n"
"    }\n"
"\n"
"    float m0 = -1e30f, l0 = 0.0f, m1 = -1e30f, l1 = 0.0f;\n"
"    float oc0[8]={0}, oc1[8]={0}, oc2[8]={0}, oc3[8]={0};\n"
"\n"
"    for (int kv = 0; kv < n_tok; kv += 16) {\n"
"        float s0[2]={0,0}, s1[2]={0,0}, s2[2]={0,0}, s3[2]={0,0};\n"
"#pragma unroll\n"
"        for (int kk = 0; kk < 64; kk += 16) {\n"
"            int ks = kk / 16;\n"
"            unsigned int a0=qa0[ks], a1=qa1[ks], a2=qa2[ks], a3=qa3[ks];\n"
"            for (int nh = 0; nh < 2; nh++) {\n"
"                int ki = kv + nh*8 + gid;\n"
"                unsigned int b0=0, b1=0;\n"
"                if (ki < n_tok) {\n"
"                    const float *kp = kt_h + ki*head_dim + kk;\n"
"                    float kf0=kp[tid4*2], kf1=kp[tid4*2+1];\n"
"                    asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(b0):\"f\"(kf0),\"f\"(kf1));\n"
"                    float kf2=kp[tid4*2+8], kf3=kp[tid4*2+9];\n"
"                    asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(b1):\"f\"(kf2),\"f\"(kf3));\n"
"                }\n"
"                asm volatile(\"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\\n\\t\"\n"
"                    \"    {%0,%1,%2,%3}, {%4,%5,%6,%7}, {%8,%9}, {%10,%11,%12,%13};\"\n"
"                    :\"=f\"(s0[nh]),\"=f\"(s1[nh]),\"=f\"(s2[nh]),\"=f\"(s3[nh])\n"
"                    :\"r\"(a0),\"r\"(a1),\"r\"(a2),\"r\"(a3),\"r\"(b0),\"r\"(b1),\n"
"                     \"f\"(s0[nh]),\"f\"(s1[nh]),\"f\"(s2[nh]),\"f\"(s3[nh]));\n"
"            }\n"
"        }\n"
"        s0[0]*=scale; s1[0]*=scale; s2[0]*=scale; s3[0]*=scale;\n"
"        s0[1]*=scale; s1[1]*=scale; s2[1]*=scale; s3[1]*=scale;\n"
"        { int c0=kv+tid4*2, c1=c0+1;\n"
"          if(c0>=n_tok){s0[0]=-1e30f;s2[0]=-1e30f;} if(c1>=n_tok){s1[0]=-1e30f;s3[0]=-1e30f;}\n"
"          if(c0+8>=n_tok){s0[1]=-1e30f;s2[1]=-1e30f;} if(c1+8>=n_tok){s1[1]=-1e30f;s3[1]=-1e30f;} }\n"
"        if(qi0>=n_tok){s0[0]=-1e30f;s1[0]=-1e30f;s0[1]=-1e30f;s1[1]=-1e30f;}\n"
"        if(qi1>=n_tok){s2[0]=-1e30f;s3[0]=-1e30f;s2[1]=-1e30f;s3[1]=-1e30f;}\n"
"\n"
"        float mx0 = fmaxf(fmaxf(s0[0],s1[0]),fmaxf(s0[1],s1[1]));\n"
"        mx0 = fmaxf(mx0, __shfl_xor_sync(0xFFFFFFFF, mx0, 1));\n"
"        mx0 = fmaxf(mx0, __shfl_xor_sync(0xFFFFFFFF, mx0, 2));\n"
"        float mn0 = fmaxf(m0, mx0);\n"
"        float al0 = expf(m0 - mn0);\n"
"        l0 *= al0; m0 = mn0;\n"
"        for (int c=0;c<8;c++) { oc0[c]*=al0; oc1[c]*=al0; }\n"
"        s0[0]=expf(s0[0]-mn0); s1[0]=expf(s1[0]-mn0);\n"
"        s0[1]=expf(s0[1]-mn0); s1[1]=expf(s1[1]-mn0);\n"
"        float rs0=s0[0]+s1[0]+s0[1]+s1[1];\n"
"        rs0+=__shfl_xor_sync(0xFFFFFFFF,rs0,1); rs0+=__shfl_xor_sync(0xFFFFFFFF,rs0,2);\n"
"        l0 += rs0;\n"
"        float mx1 = fmaxf(fmaxf(s2[0],s3[0]),fmaxf(s2[1],s3[1]));\n"
"        mx1 = fmaxf(mx1, __shfl_xor_sync(0xFFFFFFFF, mx1, 1));\n"
"        mx1 = fmaxf(mx1, __shfl_xor_sync(0xFFFFFFFF, mx1, 2));\n"
"        float mn1 = fmaxf(m1, mx1);\n"
"        float al1 = expf(m1 - mn1);\n"
"        l1 *= al1; m1 = mn1;\n"
"        for (int c=0;c<8;c++) { oc2[c]*=al1; oc3[c]*=al1; }\n"
"        s2[0]=expf(s2[0]-mn1); s3[0]=expf(s3[0]-mn1);\n"
"        s2[1]=expf(s2[1]-mn1); s3[1]=expf(s3[1]-mn1);\n"
"        float rs1=s2[0]+s3[0]+s2[1]+s3[1];\n"
"        rs1+=__shfl_xor_sync(0xFFFFFFFF,rs1,1); rs1+=__shfl_xor_sync(0xFFFFFFFF,rs1,2);\n"
"        l1 += rs1;\n"
"\n"
"        unsigned int pa0,pa1,pa2,pa3;\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa0):\"f\"(s0[0]),\"f\"(s1[0]));\n"
"#if __CUDA_ARCH__ >= 1200\n"
"        /* Blackwell sm_120+: a1/a2 fragment mapping is swapped vs sm_70-90 */\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa1):\"f\"(s2[0]),\"f\"(s3[0]));\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa2):\"f\"(s0[1]),\"f\"(s1[1]));\n"
"#else\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa1):\"f\"(s0[1]),\"f\"(s1[1]));\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa2):\"f\"(s2[0]),\"f\"(s3[0]));\n"
"#endif\n"
"        asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(pa3):\"f\"(s2[1]),\"f\"(s3[1]));\n"
"        for (int c = 0; c < 8; c++) {\n"
"            int vki0 = kv+tid4*2, vki1 = vki0+1, vki8 = vki0+8, vki9 = vki8+1;\n"
"            unsigned int vb0=0, vb1=0;\n"
"            if (vki1 < n_tok) {\n"
"                float vf0=vt_h[vki0*head_dim+c*8+gid], vf1=vt_h[vki1*head_dim+c*8+gid];\n"
"                asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(vb0):\"f\"(vf0),\"f\"(vf1));\n"
"            } else if (vki0 < n_tok) {\n"
"                float vf0=vt_h[vki0*head_dim+c*8+gid];\n"
"                asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(vb0):\"f\"(vf0),\"f\"(0.0f));\n"
"            }\n"
"            if (vki9 < n_tok) {\n"
"                float vf0=vt_h[vki8*head_dim+c*8+gid], vf1=vt_h[vki9*head_dim+c*8+gid];\n"
"                asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(vb1):\"f\"(vf0),\"f\"(vf1));\n"
"            } else if (vki8 < n_tok) {\n"
"                float vf0=vt_h[vki8*head_dim+c*8+gid];\n"
"                asm(\"cvt.rn.f16x2.f32 %0, %2, %1;\":\"=r\"(vb1):\"f\"(vf0),\"f\"(0.0f));\n"
"            }\n"
"            asm volatile(\"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\\n\\t\"\n"
"                \"    {%0,%1,%2,%3}, {%4,%5,%6,%7}, {%8,%9}, {%10,%11,%12,%13};\"\n"
"                :\"=f\"(oc0[c]),\"=f\"(oc1[c]),\"=f\"(oc2[c]),\"=f\"(oc3[c])\n"
"                :\"r\"(pa0),\"r\"(pa1),\"r\"(pa2),\"r\"(pa3),\"r\"(vb0),\"r\"(vb1),\n"
"                 \"f\"(oc0[c]),\"f\"(oc1[c]),\"f\"(oc2[c]),\"f\"(oc3[c]));\n"
"        }\n"
"    }\n"
"    float il0 = (l0>0) ? 1.0f/l0 : 0.0f;\n"
"    float il1 = (l1>0) ? 1.0f/l1 : 0.0f;\n"
"    for (int c = 0; c < 8; c++) {\n"
"        int d0 = c*8+tid4*2, d1 = d0+1;\n"
"        if (qi0<n_tok) { out[qi0*dim+h*head_dim+d0]=oc0[c]*il0; out[qi0*dim+h*head_dim+d1]=oc1[c]*il0; }\n"
"        if (qi1<n_tok) { out[qi1*dim+h*head_dim+d0]=oc2[c]*il1; out[qi1*dim+h*head_dim+d1]=oc3[c]*il1; }\n"
"    }\n"
"}\n"
"\n"
"/* ---- gelu_f32 ---- */\n"
"__global__ void gelu_f32(float *x, int n) {\n"
"    int i = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (i < n) {\n"
"        float v = x[i];\n"
"        x[i] = 0.5f * v * (1.0f + tanhf(0.7978845608f * (v + 0.044715f * v*v*v)));\n"
"    }\n"
"}\n"
"\n"
"/* ---- add_f32 ---- */\n"
"__global__ void add_f32(float *dst, const float *src, int n) {\n"
"    int i = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (i < n) dst[i] += src[i];\n"
"}\n"
"\n"
"/* ---- silu_f32 ---- */\n"
"__global__ void silu_f32(float *x, int n) {\n"
"    int i = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (i < n) {\n"
"        float v = x[i];\n"
"        x[i] = v / (1.0f + expf(-v));\n"
"    }\n"
"}\n"
"\n"
"/* ---- relu_f32 ---- */\n"
"__global__ void relu_f32(float *x, int n) {\n"
"    int i = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (i < n) x[i] = x[i] > 0 ? x[i] : 0;\n"
"}\n"
"\n"
"/* ---- resize_normalize: bilinear resize + ImageNet normalize ---- */\n"
"__global__ void resize_normalize(float *dst, const unsigned char *src,\n"
"                                  int src_w, int src_h, int dst_w, int dst_h,\n"
"                                  float mean0, float mean1, float mean2,\n"
"                                  float istd0, float istd1, float istd2) {\n"
"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    int total = dst_h * dst_w;\n"
"    if (idx >= total) return;\n"
"    int oh = idx / dst_w, ow = idx % dst_w;\n"
"    float fy = (dst_h > 1) ? (float)oh * (src_h-1) / (dst_h-1) : 0.0f;\n"
"    float fx = (dst_w > 1) ? (float)ow * (src_w-1) / (dst_w-1) : 0.0f;\n"
"    int y0 = (int)fy, x0 = (int)fx;\n"
"    int y1 = (y0+1 < src_h) ? y0+1 : y0;\n"
"    int x1 = (x0+1 < src_w) ? x0+1 : x0;\n"
"    float dy = fy - y0, dx = fx - x0;\n"
"    float mean[3] = {mean0, mean1, mean2};\n"
"    float istd[3] = {istd0, istd1, istd2};\n"
"    for (int c = 0; c < 3; c++) {\n"
"        float v = (float)src[(y0*src_w+x0)*3+c] * (1-dy)*(1-dx)\n"
"                + (float)src[(y0*src_w+x1)*3+c] * (1-dy)*dx\n"
"                + (float)src[(y1*src_w+x0)*3+c] * dy*(1-dx)\n"
"                + (float)src[(y1*src_w+x1)*3+c] * dy*dx;\n"
"        dst[c * total + idx] = (v / 255.0f - mean[c]) * istd[c];\n"
"    }\n"
"}\n"
"\n"
"/* ---- patch_embed_conv2d ---- */\n"
"__global__ void patch_embed_conv2d(float *out, const float *img, const float *w,\n"
"                                    const float *bias, int gw, int dim, int ps,\n"
"                                    int img_w) {\n"
"    int patch = blockIdx.x;\n"
"    int tid = threadIdx.x;\n"
"    int py = patch / gw, px = patch % gw;\n"
"    int tok = 1 + patch;\n"
"    for (int co = tid; co < dim; co += blockDim.x) {\n"
"        float sum = bias ? bias[co] : 0.0f;\n"
"        for (int ci = 0; ci < 3; ci++)\n"
"            for (int kh = 0; kh < ps; kh++)\n"
"                for (int kw = 0; kw < ps; kw++)\n"
"                    sum += w[((co*3+ci)*ps+kh)*ps+kw]\n"
"                         * img[ci * img_w * img_w + (py*ps+kh) * img_w + (px*ps+kw)];\n"
"        out[tok * dim + co] = sum;\n"
"    }\n"
"}\n"
"\n"
"/* ---- cls_pos_embed ---- */\n"
"__global__ void cls_pos_embed(float *hidden, const float *cls, const float *pos,\n"
"                               int n_tok, int dim) {\n"
"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    if (idx >= n_tok * dim) return;\n"
"    int t = idx / dim;\n"
"    if (t == 0)\n"
"        hidden[idx] = cls[idx] + pos[idx];\n"
"    else\n"
"        hidden[idx] += pos[idx];\n"
"}\n"
"\n"
"/* ---- bilinear_upsample_f32 ---- */\n"
"__global__ void bilinear_upsample_f32(float *dst, const float *src,\n"
"                                       int C, int Hi, int Wi, int Ho, int Wo) {\n"
"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    int total = C * Ho * Wo;\n"
"    if (idx >= total) return;\n"
"    int c = idx / (Ho * Wo);\n"
"    int rem = idx % (Ho * Wo);\n"
"    int oh = rem / Wo, ow = rem % Wo;\n"
"    float fy = (Ho > 1) ? (float)oh * (Hi-1) / (Ho-1) : 0.0f;\n"
"    float fx = (Wo > 1) ? (float)ow * (Wi-1) / (Wo-1) : 0.0f;\n"
"    int y0 = (int)fy, x0 = (int)fx;\n"
"    int y1 = (y0+1 < Hi) ? y0+1 : y0;\n"
"    int x1 = (x0+1 < Wi) ? x0+1 : x0;\n"
"    float dy = fy - y0, dx = fx - x0;\n"
"    const float *s = src + c * Hi * Wi;\n"
"    dst[idx] = s[y0*Wi+x0]*(1-dy)*(1-dx) + s[y0*Wi+x1]*(1-dy)*dx\n"
"             + s[y1*Wi+x0]*dy*(1-dx) + s[y1*Wi+x1]*dy*dx;\n"
"}\n"
"\n"
"/* ---- kv_transpose: deinterleave K,V from QKV to contiguous per-head layout ---- */\n"
"__global__ void kv_transpose(float *K_t, float *V_t, const float *qkv,\n"
"                              int n_tok, int dim, int n_heads, int head_dim) {\n"
"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n"
"    int total = n_tok * dim;\n"
"    if (idx >= total) return;\n"
"    int tok = idx / dim;\n"
"    int hd_idx = idx % dim;\n"
"    int h = hd_idx / head_dim;\n"
"    int d = hd_idx % head_dim;\n"
"    int dim3 = 3 * dim;\n"
"    int dst_idx = h * n_tok * head_dim + tok * head_dim + d;\n"
"    K_t[dst_idx] = qkv[tok * dim3 + dim + hd_idx];\n"
"    V_t[dst_idx] = qkv[tok * dim3 + 2*dim + hd_idx];\n"
"}\n"
"\n"
/* NOTE: No closing extern "C" brace here.
 * Each runner's specific kernel string must end with: } */
;

#endif /* CUDA_KERNELS_COMMON_H */
